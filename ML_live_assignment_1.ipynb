{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPioLx1epo46"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?"
      ],
      "metadata": {
        "id": "aQ2v_UdJqDCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A parameter is a value or a variable that is used to define or adjust the behavior of a system, function, or model. The specific meaning can vary depending on the context, but generally, a parameter acts as an input that influences how something operates or behaves\n",
        "\n",
        "In machine learning, parameters refer to the internal variables of a model (such as weights in a neural network) that are learned during training. These parameters define the model's output based on the input data"
      ],
      "metadata": {
        "id": "Z_YO6yFHqOV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation? What does negative correlation mean?"
      ],
      "metadata": {
        "id": "eHQf_rlgqX9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation refers to a statistical relationship between two or more variables, where changes in one variable are associated with changes in another. It quantifies the degree and direction of the relationship between these variables. Correlation is often measured using a correlation coefficient, which ranges from -1 to 1.\n",
        "\n",
        "A negative correlation between two variables means that as one variable increases, the other tends to decrease, and vice versa. This indicates an inverse relationship between the two variables. The stronger the negative correlation (closer to -1), the more consistent this inverse relationship is."
      ],
      "metadata": {
        "id": "niRJDFeqqlxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "ZpmKAqApqpjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning (ML) is a subset of artificial intelligence (AI) that enables computers to learn from data and make predictions or decisions without being explicitly programmed. Instead of following pre-written rules, a machine learning model is trained on data, allowing it to identify patterns, make predictions, or take actions based on new input.\n",
        "\n",
        "Machine learning systems typically consist of the following components:\n",
        "\n",
        "1. Data\n",
        "2. Model\n",
        "3. Algorithm\n",
        "4. optimisation\n",
        "5. Prediction\n",
        "6. Feedback"
      ],
      "metadata": {
        "id": "hh9O6sxHqylM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?"
      ],
      "metadata": {
        "id": "bPaYTjwlrD02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lower loss value indicates a better performing model, as it signifies a smaller difference between the model's predictions and the actual target values, essentially meaning the model is making fewer errors and performing closer to the desired outcome; conversely, a high loss value suggests a poorly performing model with larger prediction errors\n",
        "\n",
        "The loss function calculates the error between the predicted outputs and the actual target values. A low loss value indicates that the model's predictions are close to the actual outcomes, whereas a high loss value suggests that the model's predictions are far off\n",
        "\n",
        "The goal of most machine learning algorithms is to minimize the loss value. During the training process, optimization techniques like Gradient Descent adjust the model's parameters (weights) to reduce the loss. The lower the loss value, the better the model has \"learned\" to predict\n",
        "\n"
      ],
      "metadata": {
        "id": "szzRCFuTrIL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "M81rCj-mrr61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuous variable (also called quantitative or numerical) is a variable that can take an infinite number of values within a given range. Continuous variables are measured on a scale and can represent any value, including decimals or fractions. Examples of continuous variables include weight, height, temperature, age, and body mass.\n",
        "\n",
        "A categorical variable (also called qualitative variable) is a variable that can take on one of a limited, fixed number of possible values, which represent categories or groups. These categories might not have any meaningful order (nominal), or they may have a natural order (ordinal). Examples of categorical variables include hair color, gum flavor, dog breed, cloud type, gender, and ethnicity."
      ],
      "metadata": {
        "id": "GfrDPBSpsOLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ],
      "metadata": {
        "id": "GvTI5_kJsZT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, categorical variables (also called qualitative variables) need to be transformed into numerical representations before they can be used by most machine learning algorithms, which typically work with numeric data. The process of transforming categorical variables is crucial to ensure that algorithms can correctly interpret and process these variables.\n",
        "\n",
        "Label Encoding\n",
        "\n",
        "Label encoding is a technique where each category in a categorical variable is assigned an integer label. This is usually used for ordinal data (where the categories have a natural order) but can be applied to nominal data as well, though it's not ideal for nominal data because the model may interpret the integer values as having an ordinal relationship.\n",
        "\n",
        "One-Hot Encoding\n",
        "\n",
        "One-hot encoding is a technique used for converting nominal categorical variables into a binary (0/1) matrix. For each category, a new column is created, and the category is marked as 1 in the respective column, while all other columns are set to 0.\n",
        "\n",
        "Binary Encoding\n",
        "\n",
        "Binary encoding is a hybrid method that combines aspects of label encoding and one-hot encoding. It is particularly useful when you have a categorical variable with many unique values and want to reduce the dimensionality that one-hot encoding would introduce\n",
        "\n",
        "Target Encoding\n",
        "\n",
        "Target encoding, or mean encoding, involves encoding a categorical variable based on the mean of the target variable for each category. This method is often used in supervised learning and works well for high-cardinality categorical variables."
      ],
      "metadata": {
        "id": "IJfHIjFcsqTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "8xEyzbqQtF-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a dataset refers to the process of using a set of data (called the training data) to teach a machine learning model how to make predictions or decisions. During training, the model learns from the data by adjusting its internal parameters (or weights) to minimize errors in its predictions.\n",
        "\n",
        "Testing a dataset refers to the process of evaluating the performance of the model on a separate set of data (called the test data) that was not used during the training phase. The test data is used to simulate how the model will perform on unseen data in a real-world scenario."
      ],
      "metadata": {
        "id": "CYYjZcF7tiaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "SPaXO-9xtn_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.preprocessing is a module in Scikit-Learn, a popular Python library for machine learning, that provides tools for preprocessing data before training machine learning models. Preprocessing is a critical step in the machine learning pipeline, as raw data often needs to be transformed into a format that can be effectively used by algorithms.\n",
        "\n",
        "The preprocessing module includes various functions and classes that can help with data scaling, encoding categorical variables, normalizing features, and more. Proper preprocessing ensures that the data is in the right shape and that models perform optimally."
      ],
      "metadata": {
        "id": "x1GitkcgtrGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?"
      ],
      "metadata": {
        "id": "Ep4g3EUAt4Ks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A test set is a subset of the dataset that is used to evaluate the performance of a machine learning model after it has been trained. The key idea behind the test set is that it represents new, unseen data for the model, allowing you to assess how well the model is likely to perform on real-world data or data it has not encountered before."
      ],
      "metadata": {
        "id": "9_FxNFhbt-lx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "Q1UT4fuHuDnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, it is crucial to split your dataset into training and test sets to evaluate how well your model generalizes to unseen data. In Python, particularly using Scikit-Learn, you can use the train_test_split() function to perform this split easily.\n",
        "\n",
        "Step 1: Import Necessary Libraries\n",
        "\n",
        "Step 2: Load or Prepare Your Dataset\n",
        "\n",
        "Step 3: Split the Data\n",
        "\n",
        "Step 4: Train a Model\n",
        "\n",
        "Step 5: Evaluate the Model\n",
        "\n",
        "How to Approach a Machine Learning Problem:\n",
        "\n",
        "1. Define the Problem\n",
        "\n",
        "2. Collect and Understand Data\n",
        "\n",
        "3. Preprocess the Data\n",
        "\n",
        "4. Choose a Model\n",
        "\n",
        "5. Train the Model\n",
        "\n",
        "6. Evaluate the Model\n",
        "\n",
        "7. Tune Hyperparameters\n",
        "\n",
        "8. Model Evaluation and Interpretation\n",
        "\n",
        "9. Deploy the Model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IwSSLM_0uI6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "BAxRaPD7vVFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploratory Data Analysis (EDA) is a critical first step in the machine learning pipeline. It involves analyzing and summarizing a dataset's main characteristics, often visualizing it, before applying machine learning algorithms. EDA helps you gain a deep understanding of the data"
      ],
      "metadata": {
        "id": "YP7eGRcBvtbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python? (Q. 2,12,13 repeated)"
      ],
      "metadata": {
        "id": "tiAgD9JIvu8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, you can calculate the correlation between variables using various libraries such as Pandas and NumPy. The most common method for finding correlations is using Pearson's correlation coefficient, but there are other methods like Spearman's rank correlation and Kendall's tau correlation depending on the data characteristics."
      ],
      "metadata": {
        "id": "d_AAJagxv9Nb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example."
      ],
      "metadata": {
        "id": "KJyaBlfZwDJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Causation refers to a relationship between two events or variables where one event (or variable) directly causes the other. In other words, causation means that changes in one variable lead to changes in another variable. Causal relationships imply a cause-and-effect connection, meaning that one variable is responsible for producing a change in another variable.\n",
        "\n",
        "Causation: One variable directly causes changes in another.\n",
        "\n",
        "Correlation: Two variables are statistically related (associated) but one does not necessarily cause the other."
      ],
      "metadata": {
        "id": "ptk4Dl3wwVaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example"
      ],
      "metadata": {
        "id": "TGdUsJS-wcFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An optimizer is a mathematical algorithm used to minimize (or maximize) the objective function (also called the loss function) in machine learning models. The objective function typically measures the difference between the predicted and actual outputs, and the optimizer's goal is to minimize this difference by adjusting the model's parameters (weights and biases in neural networks).\n",
        "\n",
        "In simpler terms, an optimizer helps the model \"learn\" by finding the best possible parameters that reduce the error (loss) between predictions and actual values.\n",
        "\n",
        "Optimizers are central to training a model. They perform gradient descent (or related techniques), iteratively adjusting the model's parameters based on the gradients of the loss function with respect to the parameters.\n",
        "\n",
        "Types of optimizers:-\n",
        "\n",
        "Stochastic Gradient Descent (SGD) is a variation of Gradient Descent, where the model's parameters are updated using the gradient of the loss function for a single training example (or a small batch) rather than the entire dataset. This makes SGD much faster for large datasets, but it can lead to more noisy updates and a less stable convergence path. For a simple linear regression problem, we can update the weights using the gradient of the Mean Squared Error (MSE) loss function computed on a single data point at each step.\n",
        "\n",
        "Mini-Batch Gradient Descent is a middle ground between Batch Gradient Descent (which uses the entire dataset) and Stochastic Gradient Descent (which uses a single data point). It splits the dataset into small batches and performs updates based on the average gradient of these mini-batches. Suppose you have a dataset of 1000 samples, and you choose a batch size of 64. In each iteration, you'll update the model’s parameters using the average gradient of the loss computed over a batch of 64 samples.\n",
        "\n",
        "Momentum is an optimization technique that helps accelerate gradient descent by adding a fraction of the previous update to the current one. This helps the model converge faster and smoother, especially in the presence of noise in the gradients.\n",
        "\n"
      ],
      "metadata": {
        "id": "Cyq6Gagnwkvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "xVxyczASxRGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.linear_model is a module within the Scikit-learn library, a popular machine learning library in Python, that contains a variety of linear models for supervised learning. These models are used to make predictions based on linear relationships between the features (independent variables) and the target (dependent variable).\n",
        "\n",
        "Linear models are foundational in machine learning and can be used for both regression (predicting continuous values) and classification (predicting categorical labels). They are widely used because of their simplicity, interpretability, and efficiency."
      ],
      "metadata": {
        "id": "xPlXVcz0xUu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "d2xw1JIexeSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, model.fit() is a method used to train a machine learning model using the provided dataset. It fits the model to the data by finding the optimal parameters (such as weights and biases) that minimize the error or loss. Essentially, the fit() method is the key step in training a model. It uses the training data to \"teach\" the model how to make predictions.\n",
        "\n",
        "The arguments required by model.fit() depend on the specific machine learning algorithm you're using, but generally, there are two primary arguments:\n",
        "\n",
        "X: The input features (also called predictors, independent variables, or the feature matrix).\n",
        "\n",
        "y: The target variable (also called labels or dependent variables)."
      ],
      "metadata": {
        "id": "HOo0KxTMxlJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "jRFekQsLzFYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model.predict() method is used to make predictions with a trained machine learning model. After you have trained the model using the fit() method (which learns from the training data), you can use predict() to make predictions on new, unseen data. The method takes the input features and returns the model’s predictions (for regression) or class labels (for classification).\n",
        "\n",
        "The primary argument required by model.predict() is:\n",
        "\n",
        "X: The input features (predictors or independent variables) for the data you want to predict. This is usually a 2D array or DataFrame, where each row corresponds to one sample, and each column corresponds to one feature."
      ],
      "metadata": {
        "id": "AztCMR9MzJc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning? (Q.20 repeated)"
      ],
      "metadata": {
        "id": "Rqnc26iCzl9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is the process of standardizing or normalizing the range of independent variables or features of a dataset. In machine learning, features are the input variables that the model uses to make predictions. Feature scaling involves adjusting these features to ensure that they are on a similar scale, or at least comparable, which helps improve the performance and accuracy of machine learning algorithms.\n",
        "\n",
        "Improves Algorithm Performance: Many algorithms, such as KNN, SVM, and Gradient Descent-based models, perform better when the input features are on the same scale.\n",
        "\n",
        "Prevents Certain Features from Dominating: Without scaling, features with larger numeric ranges can dominate the model's learning process. Feature scaling prevents this by bringing all features to a comparable level.\n",
        "\n",
        "Enhances Convergence Speed: For algorithms that rely on iterative optimization methods (like gradient descent), scaling features speeds up convergence, allowing the model to learn faster.\n",
        "\n",
        "Ensures Fair Contribution: In models like Logistic Regression, Linear Regression, and neural networks, feature scaling ensures that each feature contributes equally to the model, allowing for better interpretation and decision-making."
      ],
      "metadata": {
        "id": "6GjY3Qe2zxz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "HL7tRAIo0C16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization (Min-Max Scaling)\n",
        "\n",
        "Normalization (also called Min-Max Scaling) transforms the features so that they are scaled to a specific range, usually [0, 1].\n",
        "\n",
        "Standardization (Z-Score Scaling)\n",
        "\n",
        "Standardization (also called Z-Score Scaling) transforms the features so that they have a mean of 0 and a standard deviation of 1. This is done by subtracting the mean of the feature and dividing by the standard deviation.\n",
        "\n"
      ],
      "metadata": {
        "id": "oAQEAlBI0VqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding? (Q.23 and 24 repeated)"
      ],
      "metadata": {
        "id": "YNdEXd7p0eDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data encoding is the process of converting categorical data (which is typically non-numeric, such as text labels) into a numeric format that can be used by machine learning algorithms. Since most machine learning algorithms require numerical input, encoding allows categorical features to be transformed into numerical representations.\n",
        "\n",
        "In essence, data encoding helps machine learning models understand and process non-numeric data, ensuring that they can learn meaningful patterns and make accurate predictions."
      ],
      "metadata": {
        "id": "IAtzUoap0sUt"
      }
    }
  ]
}